// Even-odd sorting (the specific order in this paraller case, algorithm usually referred to as Odd-even)
// https://www.youtube.com/watch?v=CfrEhUtGV-c

// Constants
#define SORT_THREAD_GROUP_SIZE 16
#define BATCHER_THREAD_GROUP_SIZE 2 * SORT_THREAD_GROUP_SIZE

#pragma kernel Sort
#pragma kernel BatcherMerge

uniform float3 Target;
RWBuffer<uint> Indices;
uniform StructuredBuffer<float3> Values;

groupshared uint SortPass[SORT_THREAD_GROUP_SIZE * 2];


bool ShouldSwap(uint sourceValueIndex, uint destValueIndex)
{
    float srcDist = distance(Target, Values[sourceValueIndex]);
    float destDist = distance(Target, Values[destValueIndex]);
    
    // If destination is further => swap
    return destDist < srcDist ? true : false;
}


bool isOdd(uint value)
{
    return value % 2;
}

bool isInSortBounds(uint value)
{
    return value < SORT_THREAD_GROUP_SIZE * 2 && value >= 0;
}

void SortSwap(uint sourceIndex, uint destIndex)
{
    uint swap = SortPass[sourceIndex];
    if (ShouldSwap(SortPass[sourceIndex], SortPass[destIndex]))
    {
        SortPass[sourceIndex] = SortPass[destIndex];
        SortPass[destIndex] = swap;
    }
}

// Sort kernel
[numthreads(SORT_THREAD_GROUP_SIZE, 1, 1)]
void Sort(uint3 id : SV_DispatchThreadID, uint3 GTid : SV_GroupThreadID, uint3 Gid : SV_GroupID)
{
    // TODO: Handle the last group missing some values (basically determine the groupshared actual count)
    
    // Populate groupshared memory with the 2 indices from data handled by this thread (0, 512 or 356, 868 or 511, 1023)
    uint groupIndexOffset = Gid.x * SORT_THREAD_GROUP_SIZE;
    SortPass[GTid.x] = Indices[id.x + groupIndexOffset];
    SortPass[GTid.x + SORT_THREAD_GROUP_SIZE] = Indices[id.x + groupIndexOffset + SORT_THREAD_GROUP_SIZE];
    
    // Wait for others in the group
    GroupMemoryBarrierWithGroupSync();
    
    bool isOddPass = false;
    
    // If count is odd add one because every Even-Odd cycle has to be completed
    // assume the array is perfectly dividable by thread count
    const uint passCount = SORT_THREAD_GROUP_SIZE * 2; //Count % 2 ? Count + 1 : Count;

    uint sourceIndex = 0;
    uint destIndex = 0;
    uint offset = 1;
    
    for (uint i = 0; i < passCount; i++)
    {
        sourceIndex = isOddPass ? GTid.x * 2 + 1 : GTid.x * 2;
        destIndex = sourceIndex + 1;
        
        // Handle odd pass last index from going out of bounds
        if (isOddPass && !isInSortBounds(destIndex))
        {
            sourceIndex = 0;
            destIndex = SORT_THREAD_GROUP_SIZE * 2 - 1;
        }
        
        // Try swap
        SortSwap(sourceIndex, destIndex);
    
        // Sync (make sure other threads are at the same point)
        // ONLY SYNCS THREADS IN 1 GROUP
        // BEHAVIOUR IN DYNAMIC FLOW CONTROL UNDEFINED
        // https://developer.download.nvidia.com/compute/DevZone/docs/html/DirectCompute/doc/DirectCompute_Programming_Guide.pdf
        GroupMemoryBarrierWithGroupSync();
        
        // Flip for next pass (Odd-Even)
        isOddPass = !isOddPass;
    }
    
    // Output
    Indices[id.x + groupIndexOffset] = SortPass[GTid.x];
    Indices[id.x + groupIndexOffset + SORT_THREAD_GROUP_SIZE] = SortPass[GTid.x + SORT_THREAD_GROUP_SIZE];
}

uniform bool isOddDispatch;
uniform uint groupCount;

groupshared uint BatcherPass[BATCHER_THREAD_GROUP_SIZE * 2];

void BatcherSwap(uint sourceIndex, uint destIndex)
{
    uint swap = BatcherPass[sourceIndex];
    if (ShouldSwap(BatcherPass[sourceIndex], BatcherPass[destIndex]))
    {
        BatcherPass[sourceIndex] = BatcherPass[destIndex];
        BatcherPass[destIndex] = swap;
    }
}

// TODO: FIX
// Batcher's odd-even merge kernel
// Double the size to merge 2 sorted sub arrays
// TODO: Bitonic merge sort is more optimized than the current odd-even transposition
[numthreads(BATCHER_THREAD_GROUP_SIZE, 1, 1)]
void BatcherMerge(uint3 id : SV_DispatchThreadID, uint3 GTid : SV_GroupThreadID, uint3 Gid : SV_GroupID)
{
    // TODO: Handle the last group missing some values (basically determine the groupshared actual count)
    
    // Initialize group shared memory with dispatch-level subarray offset
    // Even pass doesn't have offset because it automically mixes subarrays of 512 in the "even" manner
    // Odd pass offsets to right by subarray length and last thread group is skipped
    uint iOffset = isOddDispatch ? BATCHER_THREAD_GROUP_SIZE : 0;
    // Skip last thread group during Odd pass
    if (isOddDispatch && Gid.x == groupCount - 1)
        return;
    
    // Populate groupshared memory with the 2 indices from data handled by this thread (0, 512 or 356, 868 or 511, 1023)
    uint groupIndexOffset = Gid.x * BATCHER_THREAD_GROUP_SIZE;
    BatcherPass[GTid.x] = Indices[id.x + groupIndexOffset + iOffset];
    BatcherPass[GTid.x + BATCHER_THREAD_GROUP_SIZE] = Indices[id.x + groupIndexOffset + BATCHER_THREAD_GROUP_SIZE + iOffset];
    
    // Wait for others in the group
    GroupMemoryBarrierWithGroupSync();
    
    // Pass 1 = Compare same indices from the 2 subarrays
    // (even to even, odd to odd, example with sub arrays of length 8: 0 to 14 and 1 to 15 and 2 to 12...)
    uint subArrayLength = BATCHER_THREAD_GROUP_SIZE;
    
    uint sourceIndex = 0;
    uint destIndex = 0;
    for (subArrayLength = BATCHER_THREAD_GROUP_SIZE; subArrayLength >= 2; subArrayLength -= 2)
    {
        uint sourceOffset = BATCHER_THREAD_GROUP_SIZE - subArrayLength;
        
        sourceIndex = GTid.x + sourceOffset;
        destIndex = sourceIndex + subArrayLength;
        
        if(GTid.x < subArrayLength)
            BatcherSwap(sourceIndex, destIndex);
    
        GroupMemoryBarrierWithGroupSync();
    }
    
    // Pass 2 = Odd pass
    // (even to even, odd to odd, 1 to 3 and 2 to 4 and 5 to 7...)
    sourceIndex = (GTid.x - 1) * 2 + 1;
    destIndex = sourceIndex + 1;
    
    if (GTid.x != 0)
        BatcherSwap(sourceIndex, destIndex);
    
    GroupMemoryBarrierWithGroupSync();
    
    // Output
    Indices[id.x + groupIndexOffset + iOffset] = BatcherPass[GTid.x];
    Indices[id.x + groupIndexOffset + BATCHER_THREAD_GROUP_SIZE + iOffset] = BatcherPass[GTid.x + BATCHER_THREAD_GROUP_SIZE];
}